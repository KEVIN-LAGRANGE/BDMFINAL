# -*- coding: utf-8 -*-
"""Copy of BDM_Final_Sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DRh7FQBh6ZrMl_o5GI_0t3PSfBV_1wnm
"""

import json
import numpy as np
from pyproj import Transformer
import csv
import pandas as pd
from shapely.geometry import Point
import pyspark
from pyspark import SparkContext
import sys
from pyspark.sql import SparkSession

data = pd.read_csv('nyc_supermarkets.csv')
l1 = set(data.safegraph_placekey)
cbg = pd.read_csv('nyc_cbg_centroids.csv')
cbg['cbg_fips'] = cbg['cbg_fips'].astype('str')
l2 = set(cbg['cbg_fips'])
t = Transformer.from_crs(4326, 2263)
ll = []
for i in cbg.index:
  ll.append(t.transform(cbg.loc[i,'latitude'],cbg.loc[i,'longitude']))
cbg['loc'] = ll
loc = pd.Series(list(cbg['loc']),index = cbg['cbg_fips'])
def mapper1(id, part):
  if id==0:
    next(part)
  for line in csv.reader(part):
    poi = line[18]
    vistors = json.loads(line[19])
    if line[0] in l1:
      if (line[12][0:7] in ['2019-03','2019-10','2020-03','2020-10']) | (line[13][0:7] in ['2019-03','2019-10','2020-03','2020-10']):

        if line[12][0:7] in ['2019-03','2019-10','2020-03','2020-10']:
          date = line[12][0:7]
        else:
          date = line[13][0:7]
        for k in vistors.keys():
          if k in l2:
            distance = Point(loc[k]).distance(Point(loc[poi]))/5280
            trips = [distance] * vistors[k]
            yield ((k, date), trips)
def mapper2(x):
  m = str(round(sum(x[1])/len(x[1]),2))
  if x[0][1] == '2019-03':
    return (x[0][0],m,'','','')
  elif x[0][1] == '2019-10':
    return (x[0][0],'',m,'','')
  elif x[0][1] == '2020-03':
    return (x[0][0],'','',m,'')
  elif x[0][1] == '2020-10':
    return (x[0][0],'','','',m)
    
def main(sc):
  output1 = sc.textFile('/tmp/bdm/weekly-patterns-nyc-2019-2020/*', use_unicode=True).mapPartitionsWithIndex(mapper1) \
  .reduceByKey(lambda x,y: x+y) \
  .map(lambda x: mapper2(x))\
  .toDF(['cbg_fips', '2019-03' , '2019-10' , '2020-03' , '2020-10']) \
  .sort('cbg_fips', ascending = True)\
  .write.options(header = True).csv(sys.argv[1])

if __name__=='__main__':
  sc = SparkContext()
  main(sc)